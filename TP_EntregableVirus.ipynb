{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/ignaciomontovio/TP-Virus/blob/Nacho/TP_EntregableVirus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{},"source":["# Examen Práctico"]},{"cell_type":"markdown","metadata":{},"source":["#### 01-3900 | Ciencia de datos | 2024"]},{"cell_type":"markdown","metadata":{},"source":["Alumno:"]},{"cell_type":"markdown","metadata":{},"source":["## Enunciado"]},{"cell_type":"markdown","metadata":{},"source":["Se tienen un dataset con datos de pacientes internados en un hospital (TP_Virus_Alumnos.csv). La clase de interes (1) refiere a la presencia de un virus. El virus tiene normalmente una gravedad leve/baja y el tratamiento suele ser invasivo. Datos como nombre y apellido han sido eliminados y los valores tanto en sangre (BLD), hormonales u otros análisis sobre reactivos han sido alterados en sus valores para preservar la privacidad. Se aclara que no se ha modificado su capacidad predictiva (Si es que la tienen).\n"]},{"cell_type":"markdown","metadata":{},"source":["Para su conocimiento: </BR>\n","Datos generales de Edad, Peso, Altura y condición laboral (Activo, Pasivo etc).\n","Datos medidos en hospital:</BR>\n","BLD: Sangre</BR>\n","LVL: Hormonales</BR>\n","REC: Otros análisis</BR>\n","\n","Se pide obtener con los datos disponibles el mejor modelo posible que prediga la presencia o ausencia del virus.\n","Dado que el tratamiento es invasivo y la grevedad es moderada se requiere \"atrapar\" tantos \"1\" como sea posible y minimizar los falsos positivos para evitar que reciban un tratamiento de estas caracteristicas personas que no presentan el virus. Intente obtener el mejor modelo que maximice la métrica que considere correspondiente.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Como desarrollar el exámen"]},{"cell_type":"markdown","metadata":{},"source":["A partir del dataset realice todas las acciones para poder llegar al mejor modelo, explique brevemente en los fundamentos de sus transformaciones o acciones en general."]},{"cell_type":"markdown","metadata":{},"source":["La nota derivará de: </BR>\n","1.La calidad de la clasificación realizada</BR>\n","2.La fundamentación de los pasos realizados</BR>\n","3.Lo sencillo de llevar a producción el desarrollo</BR>\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Los docentes evaluaran su clasificador utilizando un conjunto de datos del dataset \"fuera de la caja\" (out of the box, al que usted no tiene acceso). Para minimizar la posible diferencia entre su medición y la medición del docente recuerde y aplique conceptos de test, validación cruzada y evite los errores comunes de sesgo de selección y fuga de datos (PPT/Pdf Árboles de clasificación) o  Sklearn \"10. Common pitfalls and recommended practices\" disponible en \"https://scikit-learn.org/stable/common_pitfalls.html\"   "]},{"cell_type":"markdown","metadata":{},"source":["Al final del notebook encontrará un bloque de código que lee la muestra adicional (a la que usted no tiene acceso) si PRODUCCION==True, en caso contrario solo lee una submuestra del conjunto original para validar que el código funciona. Desarrolle el notebook como considere, para finalmente asignar el mejor clasificador que usted haya obtenido remplazando en f_clf = None, None por su clasificador. Implemente todas las transformaciones entre esa línea y la predición final (Evitando al fuga de datos). Ver TP_AutomatizarTransformaciones.ipynb"]},{"cell_type":"markdown","metadata":{},"source":["En materiales del MIEL se adjunta un notebook que propone algunas ideas para automatizar el proceso."]},{"cell_type":"markdown","metadata":{},"source":["## Importaciones y Utilidades"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","## Modelos de clasificacion\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","\n","## Utilidades\n","from sklearn import metrics\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.impute import SimpleImputer,KNNImputer\n","from sklearn.model_selection import train_test_split\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.preprocessing import  MinMaxScaler\n","\n","\n","def graficarCurvaRoc( y_pred, model ):\n","  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\n","  auc = metrics.roc_auc_score(y_test, y_pred)\n","  # Graficamos\n","  plt.plot(fpr,tpr,label= model +\" AUC=\"+str(round(auc,4))) #,label= \"AUC=\"+str(auc))\n","  plt.legend(loc=4, fontsize=12)\n","  return auc\n","\n","# Para automatizar la imputacion y hacerlo mas rapido y simple\n","# Nos va a servir para probar rapido distintas strategias de imputacion, encoding, normalizacion\n","# Y ver cual es la mejor.\n","\n","class ColImputer(BaseEstimator, TransformerMixin):\n","    def __init__(self, imputer=SimpleImputer(), columns=[]):\n","        super().__init__()\n","        self.imputer = imputer\n","        self.columns = columns\n","\n","    def fit(self, X, y=None):\n","        self.imputer.fit(X[self.columns])\n","        return self\n","\n","    def get_feature_names_out(self):\n","        return self.imputer.get_feature_names_out()\n","\n","    def  transform(self, X):\n","        Xc = X.copy()\n","        Xc.loc[:, self.columns] = self.imputer.transform(Xc[self.columns])\n","        return Xc\n","\n","class ColEncoder(BaseEstimator, TransformerMixin):\n","    def __init__(self, encoder=None, columns=[]):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.columns = columns\n","\n","    def fit(self, X, y=None):\n","        self.encoder.fit(X[self.columns])\n","        return self\n","\n","    def get_feature_names_out(self):\n","        return self.get_feature_names_out()\n","\n","    def  transform(self, X):\n","        Xc = X.copy()\n","        Xc.loc[:, self.columns] = self.encoder.transform(Xc[self.columns])\n","        return Xc\n","\n","class ColScaler(BaseEstimator, TransformerMixin):\n","    def __init__(self, scaler=StandardScaler(), columns=[]):\n","        super().__init__()\n","        self.scaler = scaler\n","        self.columns = columns\n","\n","    def fit(self, X, y=None):\n","        self.scaler.fit(X[self.columns])\n","        return self\n","\n","    def get_feature_names_out(self):\n","        return self.scaler.get_feature_names_out()\n","\n","    def  transform(self, X):\n","        Xc = X.copy()\n","        Xc.loc[:, self.columns] = self.scaler.transform(Xc[self.columns])\n","        return Xc\n","\n","class ColDummy(BaseEstimator, TransformerMixin):\n","    def __init__(self, columns=[], delete = ''):\n","        super().__init__()\n","        self.columns = columns\n","        self.delete = delete\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        Xc = X.copy()\n","        Xc = pd.get_dummies(Xc, columns=self.columns)\n","        if(self.delete != ''):\n","          Xc = Xc.drop(columns=self.delete)\n","        #Xc = X.drop(columns=self.columns)\n","        return Xc\n","\n","class ReplaceValue(BaseEstimator, TransformerMixin):\n","    def __init__(self, column, old_value, new_value):\n","        self.column = column\n","        self.old_value = old_value\n","        self.new_value = new_value\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        Xc = X.copy()\n","        Xc.loc[:, self.column] = Xc[self.column].replace(self.old_value, self.new_value)\n","        return Xc\n","    \n","class ColumnTypeModifier:\n","    def __init__(self, column_name, new_dtype):\n","        self.column_name = column_name\n","        self.new_dtype = new_dtype\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        Xc = X.copy()\n","        Xc[self.column_name] = Xc[self.column_name].astype(self.new_dtype)\n","        return Xc\n","    \n","class ColumnDrop:\n","    def __init__(self, columns_to_drop=[]):\n","        super().__init__()\n","        self.columns_to_drop=columns_to_drop\n","\n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X):\n","        Xc = X.copy()\n","        Xc = Xc.drop(self.columns_to_drop, axis=1)\n","        return Xc"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluacion final - Docente + Alumno"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Primero necesitamos hacer una ejecucion de las celdas de abajo ( tratamiento de variables, evaluacion y mejora de modelos ) para establecer el modelo antes de ir a produccion.\n","\n","PRODUCCION = False\n","\n","#Leemos el dataset de evaluación, simulando producción\n","if not PRODUCCION:\n","    df = pd.read_csv(\"TP_Virus_Alumnos.csv\")\n","    # Seguir ejecutando demas celdas del cuaderno.\n","\n","else:\n","\n","    df = pd.read_csv(\"TP_Virus_Evaluacion.csv\")\n","\n","    #Dividimos en target y predictoras\n","    X_prod = df.drop(\"target\", axis=1)\n","    y_prod = df[\"target\"]\n","\n","    # Treaemos el modelo de Random Forest obtenido previamente en grid search si existe o en su defecto el modelo sin optimizar\n","    best_clf = best_rf if best_rf else rf_model\n","\n","    display(best_clf)\n","    \n","    #Transformaciones\n","    X_prod = pl.transform(X_prod)\n","\n","    #Evaluación final\n","    y_pred = best_clf.predict(X_prod)\n","    print(classification_report(y_prod, y_pred))\n","    df.shape[0]\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Analisis de Variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# verificamos los tipos de datos\n","df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Verificamos si hay valores nulos para imputar\n","print(df.isnull().sum())\n","# Verificamos si hay duplicados\n","print(df.duplicated().sum())"]},{"cell_type":"markdown","metadata":{},"source":["### Correlacion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#En la matriz de correlación vemos que edad, peso e hijos estan ampliamente correlacionados.\n","#Las demas variables no poseen correlacion entre ellas, pueden aportar valor.\n","fig, ax = plt.subplots(figsize=(10,7))\n","mat = df.select_dtypes(include=['int', 'float']).corr()\n","sns.heatmap(mat, annot=True, cmap='coolwarm', fmt=\".2f\",  ax=ax)\n","plt.title('Matriz de Correlación')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Edad"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Analisis de la distribución de la variable target \"Edad\"\n","print( df.Edad.value_counts() )\n","sns.countplot(x='Edad', data=df, hue='target', legend=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"Edad\"]].boxplot()\n","df[[\"Edad\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"Edad\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.Edad.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### Peso"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"Peso\"]].boxplot()\n","df[[\"Peso\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"Peso\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.Peso.value_counts()\n","\n","# Observamos outliers en peso. Los valores atipicos estan relacionados con los recien nacidos, debido a la media del peso de la muestra,\n","# parece ser un valor fuera de lo normal cuando no lo es. Sin embargo que el peso '8.934178..'\n","# sea el único que encuentre repetido varias veces es indicativo de un error en la carga o defectos en la muestra."]},{"cell_type":"markdown","metadata":{},"source":["### Hijos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Analisis de la distribución de la variable target \"hijos\"\n","print( df.hijos.value_counts() )\n","sns.countplot(x='hijos', data=df, hue='hijos', legend=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"hijos\"]].boxplot()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"hijos\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"hijos\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.hijos.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### BLD01"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"BLD01\"]].boxplot()\n","df[[\"BLD01\"]].hist(bins=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"BLD01\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.BLD01.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### BLD02"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"BLD02\"]].boxplot()\n","df[[\"BLD02\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"BLD02\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df.BLD02.value_counts())"]},{"cell_type":"markdown","metadata":{},"source":["### BLD03"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"BLD03\"]].boxplot()\n","df[[\"BLD03\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"BLD03\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.BLD03.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### REC1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC1\"]].boxplot()\n","df[[\"REC1\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC1\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.REC1.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### REC2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC2\"]].boxplot()\n","df[[\"REC2\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC2\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.REC2.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### REC3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC3\"]].boxplot()\n","df[[\"REC3\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC3\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.REC3.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### REC4"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC4\"]].boxplot()\n","df[[\"REC4\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC4\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.REC4.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### REC5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC5\"]].boxplot()\n","df[[\"REC5\"]].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"REC5\"]].describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.REC5.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### Target"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.countplot(x='target', data=df, hue='target', legend=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#No hay duplicados.\n","print(\"Cantidad:\",  df.duplicated().sum())"]},{"cell_type":"markdown","metadata":{},"source":["## Tratamiento de variables\n","\n","Aca vamos a tratar cosas generales que queremos aplicar siempre, luego en la evaluacion del modelo,\n","con los pipelines probamos y analiamos distintas estragias de imputacion, normalizacion, etc..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_norm = df\n","\n","X = df_norm.drop(\"target\", axis=1)\n","y = df_norm[\"target\"]\n","\n","X_train_norm, X_test_norm, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.3)\n","\n","\n","pl = Pipeline(steps=[\n","    (\"DropColumns\", ColumnDrop([\"Genero\", \"hijos\", \"Laboral\", \"REC1\", \"REC2\"])),\n","    (\"BLD03Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD03\"])),\n","    (\"BLD02Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD02\"])),\n","    (\"BLD01Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD01\"])),\n","    (\"LVLReplace\", ReplaceValue(\"LVL\",1000000, np.nan)), # Reemplazar los 1000000 por nan\n","    (\"LVLImputer\", ColImputer(imputer=SimpleImputer(strategy='mean'), columns=[\"LVL\"])),\n","    (\"LVLScaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"LVL\"])),\n","    (\"EdadImputer\", ColImputer(imputer=SimpleImputer(strategy='median'), columns=[\"Edad\"])),\n","    (\"EdadTypeReplace\",ColumnTypeModifier(\"Edad\",int))\n","])\n","\n","pl.fit(X_train_norm, y_train)\n","X_train = pl.transform(X_train_norm)\n","X_test = pl.transform(X_test_norm)\n","\n","X_train"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluacion de modelos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logreg = LogisticRegression( max_iter=3000 )\n","logreg.fit(X_train, y_train)\n","y_pred_lg = logreg.predict(X_test)\n","\n","treeclf = DecisionTreeClassifier(max_depth=10, random_state=1)\n","treeclf.fit(X_train, y_train)\n","y_pred_tc = treeclf.predict(X_test)\n","\n","bayes_multi = MultinomialNB()\n","bayes_multi.fit(X_train, y_train)\n","y_pred_nb = bayes_multi.predict(X_test)\n","\n","bayes_gauss = GaussianNB()\n","bayes_gauss.fit(X_train, y_train)\n","y_pred_gauss = bayes_gauss.predict(X_test)\n","\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","y_pred_rf = rf_model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Inicializamos los labels del gráfico\n","plt.figure(figsize=(20, 10))\n","plt.xlabel('% 1 – Specificity (falsos positivos)', fontsize=14)\n","plt.ylabel('% Sensitivity (positivos)', fontsize=14)\n","\n","# Graficamos la recta del azar\n","it = [i/100 for i in range(100)]\n","plt.plot(it,it,label=\"AZAR AUC=0.5\",color=\"black\")\n","\n","modelos = { 'bayesGauss':y_pred_gauss,'arbol':y_pred_tc, 'reglog':y_pred_lg, 'multinomial': y_pred_nb, 'randomForest':y_pred_rf}\n","areas = []\n","for pred in modelos:\n","    auc = graficarCurvaRoc( modelos[pred] , pred )\n","    areas.append( (pred, auc) )\n","areas = pd.DataFrame(areas, columns=['model','auc'])\n","areas.sort_values('auc', ascending=False)\n","\n","cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test,y_pred_rf), display_labels = [False, True])\n","cm_display.plot()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Clasificacion de todos los modelos.\n","Debemos mejorar precision: para evitar los falsos positivos, dado que el tratamiento es invasivo y la gravedad moderada.\n","Debemos mejorar recall: porque queremos detectar todos los positivos posibles."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import display, Markdown\n","\n","display(Markdown(f\"## LogisticRegression\"))\n","print(classification_report(y_test, y_pred_lg))\n","\n","display(Markdown(f\"## DecisionTreeClassifier\"))\n","print(classification_report(y_test, y_pred_tc))\n","\n","display(Markdown(f\"## MultinomialNB\"))\n","print(classification_report(y_test, y_pred_nb))\n","\n","display(Markdown(f\"## GaussianNB\"))\n","print(classification_report(y_test, y_pred_gauss))\n","\n","display(Markdown(f\"## RandomForest\"))\n","print(classification_report(y_test, y_pred_rf))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Mejora de los modelos con GridSearch"]},{"cell_type":"markdown","metadata":{},"source":["#### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from warnings import simplefilter\n","# ignore all warnings\n","simplefilter(action='ignore')\n","\n","parameters =  {\"C\":np.logspace(-3,3,13), \"penalty\":[\"l1\",\"l2\",None], \"max_iter\":[1500,3000,4000]}\n","clf = GridSearchCV( LogisticRegression() , parameters, scoring='precision', cv=5, )\n","clf.fit(X_train, y_train)\n","\n","print(\"Best Parameters:\", clf.best_params_)\n","print(\"Best Score:\", clf.best_score_)\n","\n","best_logreg = clf.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["#### Decision Tree Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parameters = {\"criterion\": ['gini', 'entropy', 'log_loss'], \"splitter\": ['best', 'random'], \"max_depth\": [2, 5, 10, 20, 30, 40], \"random_state\": [1,5,9,15,20,30,40]}\n","clf = GridSearchCV( DecisionTreeClassifier() , parameters, scoring='precision', cv=5, )\n","clf.fit(X_train, y_train)\n","\n","print(\"Best Parameters:\", clf.best_params_)\n","print(\"Best Score:\", clf.best_score_)\n","\n","best_decisiontree = clf.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["#### Multinomial"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parameters = {\"fit_prior\": [True, False], \"alpha\": [0,1.0,2.0,3.0], \"force_alpha\": [True, False]}\n","clf = GridSearchCV( MultinomialNB() , parameters, scoring='precision', cv=5, )\n","clf.fit(X_train, y_train)\n","\n","print(\"Best Parameters:\", clf.best_params_)\n","print(\"Best Score:\", clf.best_score_)\n","\n","best_multibayes = clf.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["#### GaussianNB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parameters = {'var_smoothing': np.logspace(0,-9, num=100)}\n","clf = GridSearchCV( GaussianNB() , parameters, scoring='precision', cv=5, )\n","clf.fit(X_train, y_train)\n","\n","print(\"Best Parameters:\", clf.best_params_)\n","print(\"Best Score:\", clf.best_score_)\n","\n","best_gaussbayes = clf.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["#### Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parameters = {\n","    'n_estimators': [100],  # Número de árboles en el bosque\n","    'max_depth': [None],  # Máxima profundidad de los árboles\n","    'min_samples_split': [2, 5],  # Número mínimo de muestras requeridas para dividir un nodo\n","    'min_samples_leaf': [1, 2],  # Número mínimo de muestras requeridas en un nodo hoja\n","    'bootstrap': [True], \n","    \"random_state\": [1,5,9,15,20,30,40] # Método para seleccionar muestras para entrenar cada árbol\n","    }\n","\n","clf = GridSearchCV(RandomForestClassifier(), parameters, scoring='accuracy', cv=5)\n","clf.fit(X_train, y_train)\n","\n","# Imprimir los mejores parámetros y el mejor puntaje\n","print(\"Best Parameters:\", clf.best_params_)\n","print(\"Best Score:\", clf.best_score_)\n","\n","# Obtener el mejor modelo\n","best_rf = clf.best_estimator_ "]},{"cell_type":"markdown","metadata":{},"source":["### Re-Evaluacion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_logreg.fit(X_train, y_train)\n","y_pred_lg = best_logreg.predict(X_test)\n","\n","\n","best_decisiontree.fit(X_train, y_train)\n","y_pred_tc = best_decisiontree.predict(X_test)\n","\n","best_multibayes.fit(X_train, y_train)\n","y_pred_nb = best_multibayes.predict(X_test)\n","\n","best_gaussbayes.fit(X_train, y_train)\n","y_pred_gauss = best_gaussbayes.predict(X_test)\n","\n","best_rf.fit(X_train, y_train)\n","y_pred_rf = best_rf.predict(X_test)\n","\n","plt.figure(figsize=(20, 10))\n","plt.xlabel('% 1 – Specificity (falsos positivos)', fontsize=14)\n","plt.ylabel('% Sensitivity (positivos)', fontsize=14)\n","\n","# Graficamos la recta del azar\n","it = [i/100 for i in range(100)]\n","plt.plot(it,it,label=\"AZAR AUC=0.5\",color=\"black\")\n","\n","modelos = { 'randomForest': y_pred_rf ,'bayesGauss':y_pred_gauss,'arbol':y_pred_tc, 'reglog':y_pred_lg, 'multinomial': y_pred_nb}\n","areas = []\n","for pred in modelos:\n","    auc = graficarCurvaRoc( modelos[pred] , pred )\n","    areas.append( (pred, auc) )\n","areas = pd.DataFrame(areas, columns=['model','auc'])\n","# Grafico\n","# plt.title(\"Curva ROC\", fontsize=14)\n","# plt.tick_params(labelsize=12);\n","# plt.show()\n","# Tabla\n","areas.sort_values('auc', ascending=False)\n","\n","confusion_matrix(y_test,y_pred_rf)\n","cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test,y_pred_rf), display_labels = [False, True])\n","cm_display.plot()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import display, Markdown\n","\n","display(Markdown(f\"## RandomForest\"))\n","print(classification_report(y_test, y_pred_rf))\n","\n","display(Markdown(f\"## LogisticRegression\"))\n","print(classification_report(y_test, y_pred_lg))\n","\n","display(Markdown(f\"## DecisionTreeClassifier\"))\n","print(classification_report(y_test, y_pred_tc))\n","\n","display(Markdown(f\"## MultinomialNB\"))\n","print(classification_report(y_test, y_pred_nb))\n","\n","display(Markdown(f\"## GaussianNB\"))\n","print(classification_report(y_test, y_pred_gauss))"]},{"cell_type":"markdown","metadata":{},"source":["## Validación Cruzada\n","\n","Para garantizar la robustez y la generalización de nuestro modelo utilizaremos la técnica de validación cruzada (K-Folds). Esto nos permitirá evaluar el rendimiento del modelo de manera más exhaustiva.\n","\n","La validación cruzada nos ayudará a reducir el sesgo de evaluación,Además nos permitirá identificar la variabilidad en el rendimiento del modelo, minimizando así el riesgo de sobreajuste (overfitting) y asegurando que el modelo generalice bien a nuevos datos no vistos."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","\n","def realizar_kfolds_val(model,X,y,splits=5,shf=True):\n","    kf = KFold(n_splits=splits, shuffle=shf, random_state=42)\n","\n","    # Evaluar el modelo utilizando validación cruzada\n","    scores = cross_val_score(model, X, y, cv=kf)\n","\n","    display(Markdown(f' Scores for each fold: \\n {scores}'))\n","    display(Markdown(f' Mean score: \\n {scores.mean()}'))\n","    display(Markdown(f' Standard deviation: \\n {scores.std()}'))\n","\n","\n","folds = 10\n","\n","display(Markdown(\"# Cross-Validation - Log Reg\"))\n","realizar_kfolds_val(best_logreg,X_train,y_train,folds)\n","\n","display(Markdown(\"\\n\\n # Cross-Validation - Decision Tree Classifier\"))\n","realizar_kfolds_val(best_decisiontree,X_train,y_train,folds)\n","\n","display(Markdown(\"# Cross-Validation - Random Forest Classifier\"))\n","realizar_kfolds_val(best_rf,X_train,y_train,folds)\n","\n","display(Markdown(\"\\n\\n # Cross-Validation - Multinomial Bayes\"))\n","realizar_kfolds_val(best_multibayes,X_train,y_train,folds)\n","\n","display(Markdown(\"\\n\\n # Cross-Validation - Gaussian Bayes\"))\n","realizar_kfolds_val(best_gaussbayes,X_train,y_train,folds)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":0}
