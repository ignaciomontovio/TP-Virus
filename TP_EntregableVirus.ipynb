{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ignaciomontovio/TP-Virus/blob/Nacho/TP_EntregableVirus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Ht02FO2qB4"
      },
      "source": [
        "# Examen Práctico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfqt2DW42qB7"
      },
      "source": [
        "#### 01-3900 | Ciencia de datos | 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6hj8Kvc2qB8"
      },
      "source": [
        "Alumno:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6LptPSN2qB8"
      },
      "source": [
        "## Enunciado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDxRU1EE2qB8"
      },
      "source": [
        "Se tienen un dataset con datos de pacientes internados en un hospital (TP_Virus_Alumnos.csv). La clase de interes (1) refiere a la presencia de un virus. El virus tiene normalmente una gravedad leve/baja y el tratamiento suele ser invasivo. Datos como nombre y apellido han sido eliminados y los valores tanto en sangre (BLD), hormonales u otros análisis sobre reactivos han sido alterados en sus valores para preservar la privacidad. Se aclara que no se ha modificado su capacidad predictiva (Si es que la tienen).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI4C9xTc2qB9"
      },
      "source": [
        "Para su conocimiento: </BR>\n",
        "Datos generales de Edad, Peso, Altura y condición laboral (Activo, Pasivo etc).\n",
        "Datos medidos en hospital:</BR>\n",
        "BLD: Sangre</BR>\n",
        "LVL: Hormonales</BR>\n",
        "REC: Otros análisis</BR>\n",
        "\n",
        "Se pide obtener con los datos disponibles el mejor modelo posible que prediga la presencia o ausencia del virus.\n",
        "Dado que el tratamiento es invasivo y la grevedad es moderada se requiere \"atrapar\" tantos \"1\" como sea posible y minimizar los falsos positivos para evitar que reciban un tratamiento de estas caracteristicas personas que no presentan el virus. Intente obtener el mejor modelo que maximice la métrica que considere correspondiente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2wmDxmV2qB9"
      },
      "source": [
        "## Como desarrollar el exámen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQtuYVef2qB9"
      },
      "source": [
        "A partir del dataset realice todas las acciones para poder llegar al mejor modelo, explique brevemente en los fundamentos de sus transformaciones o acciones en general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h134BZGO2qB9"
      },
      "source": [
        "La nota derivará de: </BR>\n",
        "1.La calidad de la clasificación realizada</BR>\n",
        "2.La fundamentación de los pasos realizados</BR>\n",
        "3.Lo sencillo de llevar a producción el desarrollo</BR>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxB5wX7h2qB-"
      },
      "source": [
        "Los docentes evaluaran su clasificador utilizando un conjunto de datos del dataset \"fuera de la caja\" (out of the box, al que usted no tiene acceso). Para minimizar la posible diferencia entre su medición y la medición del docente recuerde y aplique conceptos de test, validación cruzada y evite los errores comunes de sesgo de selección y fuga de datos (PPT/Pdf Árboles de clasificación) o  Sklearn \"10. Common pitfalls and recommended practices\" disponible en \"https://scikit-learn.org/stable/common_pitfalls.html\"   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj2eDmfv2qB-"
      },
      "source": [
        "Al final del notebook encontrará un bloque de código que lee la muestra adicional (a la que usted no tiene acceso) si PRODUCCION==True, en caso contrario solo lee una submuestra del conjunto original para validar que el código funciona. Desarrolle el notebook como considere, para finalmente asignar el mejor clasificador que usted haya obtenido remplazando en f_clf = None, None por su clasificador. Implemente todas las transformaciones entre esa línea y la predición final (Evitando al fuga de datos). Ver TP_AutomatizarTransformaciones.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVc4LBk52qB-"
      },
      "source": [
        "En materiales del MIEL se adjunta un notebook que propone algunas ideas para automatizar el proceso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eNhohjT2qB-"
      },
      "source": [
        "## Evaluacion final - Docente + Alumno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4heu2ao12qB_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
        "\n",
        "\n",
        "PRODUCCION = False\n",
        "best_clf = None #Asignar aqui el mejor clasificador posible (previamente entrenado)\n",
        "\n",
        "#Leemos el dataset de evaluación, simulando producción\n",
        "if PRODUCCION==False:\n",
        "    df = pd.read_csv(\"TP_Virus_Alumnos.csv\")\n",
        "    # df_orig = df\n",
        "    # _, df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "else:\n",
        "    df = pd.read_csv(\"TP_Virus_Evaluacion.csv\")\n",
        "#Dividimos en target y predictoras\n",
        "\n",
        "X_prod = df.drop(\"target\", axis=1)\n",
        "y_prod = df[\"target\"]\n",
        "\n",
        "#Transformaciones\n",
        "\n",
        "\n",
        "#Evaluación final\n",
        "#y_pred = best_clf.predict(X_prod)\n",
        "#print(classification_report(y_prod, y_pred))\n",
        "# df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW0dXfFZZNYj"
      },
      "source": [
        "## Analisis de Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# verificamos los tipos de datos\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificamos si hay valores nulos para imputar\n",
        "print(df.isnull().sum())\n",
        "# Verificamos si hay duplicados\n",
        "print(df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm8XZWEpZNYk"
      },
      "source": [
        "### Correlacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "R7wQO2EZN1gP",
        "outputId": "3a75e0d1-8f85-4fbe-ff9f-67ebee93ef0e"
      },
      "outputs": [],
      "source": [
        "#En la matriz de correlación vemos que edad, peso e hijos estan ampliamente correlacionados.\n",
        "#Las demas variables no poseen correlacion entre ellas, pueden aportar valor.\n",
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "mat = df.select_dtypes(include=['int', 'float']).corr()\n",
        "sns.heatmap(mat, annot=True, cmap='coolwarm', fmt=\".2f\",  ax=ax)\n",
        "plt.title('Matriz de Correlación')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsJk9C0uZNYk"
      },
      "source": [
        "### Edad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_uGMDnNZNYk"
      },
      "outputs": [],
      "source": [
        "# Analisis de la distribución de la variable target \"Edad\"\n",
        "print( df.Edad.value_counts() )\n",
        "sns.countplot(x='Edad', data=df, hue='target', legend=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nS5zZg7ZNYl"
      },
      "outputs": [],
      "source": [
        "df[[\"Edad\"]].boxplot()\n",
        "df[[\"Edad\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT-CnqLRZNYl"
      },
      "outputs": [],
      "source": [
        "df[[\"Edad\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ8QHFCOZNYl"
      },
      "outputs": [],
      "source": [
        "df.Edad.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ_3I37ZZNYl"
      },
      "source": [
        "### Peso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSLhFS42ZNYl"
      },
      "outputs": [],
      "source": [
        "df[[\"Peso\"]].boxplot()\n",
        "df[[\"Peso\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hfWKdWJZNYm"
      },
      "outputs": [],
      "source": [
        "df[[\"Peso\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWaGFOIeZNYm"
      },
      "outputs": [],
      "source": [
        "df.Peso.value_counts()\n",
        "\n",
        "# Observamos outliers en peso. Los valores atipicos estan relacionados con los recien nacidos, debido a la media del peso de la muestra,\n",
        "# parece ser un valor fuera de lo normal cuando no lo es. Sin embargo que el peso '8.934178..'\n",
        "# sea el único que encuentre repetido varias veces es indicativo de un error en la carga o defectos en la muestra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpuGz_48ZNYm"
      },
      "source": [
        "### Hijos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-H4R78AZNYm"
      },
      "outputs": [],
      "source": [
        "# Analisis de la distribución de la variable target \"hijos\"\n",
        "print( df.hijos.value_counts() )\n",
        "sns.countplot(x='hijos', data=df, hue='hijos', legend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG_afx84ZNYm"
      },
      "source": [
        "df[[\"hijos\"]].boxplot()\n",
        "df[[\"hijos\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igkS5nayZNYm"
      },
      "outputs": [],
      "source": [
        "df[[\"hijos\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZNFTwMzZNYn"
      },
      "outputs": [],
      "source": [
        "df.hijos.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP9rfJryZNYn"
      },
      "source": [
        "### BLD01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp4Ce5gOZNYn"
      },
      "outputs": [],
      "source": [
        "df[[\"BLD01\"]].boxplot()\n",
        "df[[\"BLD01\"]].hist(bins=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSdaFN-4ZNYn"
      },
      "outputs": [],
      "source": [
        "df[[\"BLD01\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYeQArVjZNYn"
      },
      "outputs": [],
      "source": [
        "df.BLD01.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uO_nYFfZNYo"
      },
      "source": [
        "### BLD02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T4QCoL1ZNYo"
      },
      "outputs": [],
      "source": [
        "df[[\"BLD02\"]].boxplot()\n",
        "df[[\"BLD02\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTLIdwj7ZNYo"
      },
      "outputs": [],
      "source": [
        "df[[\"BLD02\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6463MQGiZNYo"
      },
      "outputs": [],
      "source": [
        "print(df.BLD02.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJwj9LT0ZNYo"
      },
      "source": [
        "### BLD03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1zDC6OJZNYp"
      },
      "outputs": [],
      "source": [
        "df[[\"BLD03\"]].boxplot()\n",
        "df[[\"BLD03\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgXLuP0-ZNYp"
      },
      "outputs": [],
      "source": [
        "df[[\"BLD03\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO3Nur_1ZNYp"
      },
      "outputs": [],
      "source": [
        "df.BLD03.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B9sxoeQZNYp"
      },
      "source": [
        "### REC1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUl04bghZNYp"
      },
      "outputs": [],
      "source": [
        "df[[\"REC1\"]].boxplot()\n",
        "df[[\"REC1\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_qHanK1ZNYq"
      },
      "outputs": [],
      "source": [
        "df[[\"REC1\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrhGE_RGZNYq"
      },
      "outputs": [],
      "source": [
        "df.REC1.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JDReYxvZNYq"
      },
      "source": [
        "### REC2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOWAbhpJZNYq"
      },
      "outputs": [],
      "source": [
        "df[[\"REC2\"]].boxplot()\n",
        "df[[\"REC2\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCJHo9ACZNYq"
      },
      "outputs": [],
      "source": [
        "df[[\"REC2\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiMsR9xpZNYr"
      },
      "outputs": [],
      "source": [
        "df.REC2.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiPor8GyZNYr"
      },
      "source": [
        "### REC3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFiTFqlZNYr"
      },
      "outputs": [],
      "source": [
        "df[[\"REC3\"]].boxplot()\n",
        "df[[\"REC3\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GlizHDNZNYr"
      },
      "outputs": [],
      "source": [
        "df[[\"REC3\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfUuf5LKZNYs"
      },
      "outputs": [],
      "source": [
        "df.REC3.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h5isSBuZNYs"
      },
      "source": [
        "### REC4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WHpZ0DlZNYs"
      },
      "outputs": [],
      "source": [
        "df[[\"REC4\"]].boxplot()\n",
        "df[[\"REC4\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8wg6nRpZNYs"
      },
      "outputs": [],
      "source": [
        "df[[\"REC4\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV_01fRrZNYs"
      },
      "outputs": [],
      "source": [
        "df.REC4.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10t2itS8ZNYt"
      },
      "source": [
        "### REC5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7yeaEEWZNYt"
      },
      "outputs": [],
      "source": [
        "df[[\"REC5\"]].boxplot()\n",
        "df[[\"REC5\"]].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q--n5kfzZNYt"
      },
      "outputs": [],
      "source": [
        "df[[\"REC5\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnla3q7WZNYt"
      },
      "outputs": [],
      "source": [
        "df.REC5.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.countplot(x='target', data=df, hue='target', legend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWa9AwZ_ZNYt"
      },
      "source": [
        "### Conclusiones y preguntas\n",
        "\n",
        "Las variables BLD01, BLD02, BLD03, REC1, REC2, REC3, REC4 y REC5, tienen outliers casi en ambos extremos, dice en el enunciado que estas variables fueron alteradas en sus valores para preservar la privacidad de las personas. Quizas eso afecto su distribución?\n",
        "Estoy dudando si eliminarlas o tratarlas porque las variables Peso y LVL tambien muestran outliers pero en sus limites inferiores. Que se explica con que la muestra contiene recien nacidos y personas muy jovenes, por lo que no creo que se necesite tratarlas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDdpNUKhZNYu"
      },
      "outputs": [],
      "source": [
        "#No hay duplicados.\n",
        "print(\"Cantidad:\",  df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu4a6ZeCZNYu"
      },
      "source": [
        "## Tratamiento de variables\n",
        "\n",
        "Aca vamos a tratar cosas generales que queremos aplicar siempre, luego en la evaluacion del modelo,\n",
        "con los pipelines probamos y analiamos distintas estragias de imputacion, normalizacion, etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnBSRvoKihqo"
      },
      "outputs": [],
      "source": [
        "#Elimino los nulos en LVL\n",
        "df_normalize = df.dropna(subset=['LVL'])\n",
        "df_normalize = df_normalize.dropna(subset=['Edad'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R22sWZRXS9nf"
      },
      "outputs": [],
      "source": [
        "def preparacion_de_datos(df_input, target, escalar_valores=False):\n",
        "  df = df_input\n",
        "  if escalar_valores:\n",
        "    scaler_X = StandardScaler(with_mean=True, with_std=True)\n",
        "    scaler_X.fit(df.drop(target,axis=1))\n",
        "    x = pd.DataFrame(scaler_X.transform(df.drop(target,axis=1),), columns = df.drop(target,axis=1).columns )\n",
        "  else:\n",
        "    x = df.drop(target,axis=1)\n",
        "  y = df[target]\n",
        "  return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHYXQ68oTNwn"
      },
      "outputs": [],
      "source": [
        "X_virus, y_virus = preparacion_de_datos( df_normalize, \"target\" ,escalar_valores=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aozN57jDTT6E"
      },
      "outputs": [],
      "source": [
        "# Hacemos el Split 70-30 para train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_virus, y_virus,\n",
        "                                                    test_size=0.3, stratify = y_virus, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvou-hLGZNYx"
      },
      "source": [
        "## Utilidades - Imputacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def graficarCurvaRoc( y_pred, model ):\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "  # Graficamos\n",
        "  plt.plot(fpr,tpr,label= model +\" AUC=\"+str(round(auc,4))) #,label= \"AUC=\"+str(auc))\n",
        "  plt.legend(loc=4, fontsize=12)\n",
        "  return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uyvE2YzZNYy"
      },
      "outputs": [],
      "source": [
        "# Para automatizar la imputacion y hacerlo mas rapido y simple\n",
        "# Nos va a servir para probar rapido distintas strategias de imputacion, encoding, normalizacion\n",
        "# Y ver cual es la mejor.\n",
        "\n",
        "class ColImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, imputer=SimpleImputer(), columns=[]):\n",
        "        super().__init__()\n",
        "        self.imputer = imputer\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.imputer.fit(X[self.columns])\n",
        "        return self\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return self.imputer.get_feature_names_out()\n",
        "\n",
        "    def  transform(self, X):\n",
        "        Xc = X.copy()\n",
        "        Xc.loc[:, self.columns] = self.imputer.transform(Xc[self.columns])\n",
        "        return Xc\n",
        "\n",
        "class ColEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, encoder=None, columns=[]):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.encoder.fit(X[self.columns])\n",
        "        return self\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return self.get_feature_names_out()\n",
        "\n",
        "    def  transform(self, X):\n",
        "        Xc = X.copy()\n",
        "        Xc.loc[:, self.columns] = self.encoder.transform(Xc[self.columns])\n",
        "        return Xc\n",
        "\n",
        "class ColScaler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, scaler=StandardScaler(), columns=[]):\n",
        "        super().__init__()\n",
        "        self.scaler = scaler\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.scaler.fit(X[self.columns])\n",
        "        return self\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return self.scaler.get_feature_names_out()\n",
        "\n",
        "    def  transform(self, X):\n",
        "        Xc = X.copy()\n",
        "        Xc.loc[:, self.columns] = self.scaler.transform(Xc[self.columns])\n",
        "        return Xc\n",
        "\n",
        "class ColDummy(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns=[], delete = ''):\n",
        "        super().__init__()\n",
        "        self.columns = columns\n",
        "        self.delete = delete\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Xc = X.copy()\n",
        "        Xc = pd.get_dummies(Xc, columns=self.columns)\n",
        "        if(self.delete != ''):\n",
        "          Xc = Xc.drop(columns=self.delete)\n",
        "        #Xc = X.drop(columns=self.columns)\n",
        "        return Xc\n",
        "\n",
        "class ReplaceValue(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, old_value, new_value):\n",
        "        self.column = column\n",
        "        self.old_value = old_value\n",
        "        self.new_value = new_value\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Xc = X.copy()\n",
        "        Xc.loc[:, self.column] = Xc[self.column].replace(self.old_value, self.new_value)\n",
        "        return Xc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3fRXDUUZNY0"
      },
      "source": [
        "## Evaluacion de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "phtR6l04ZNY0",
        "outputId": "d122c3b4-534f-4203-aa88-16688dd336d7"
      },
      "outputs": [],
      "source": [
        "df_norm = df\n",
        "df_norm.loc[df['LVL'] == 1000000, 'LVL'] = None\n",
        "\n",
        "#df_norm = df_norm[df_norm['Edad'] > 10]\n",
        "#df_norm = df_norm[df_norm['Peso'] != 8.934178194834107]\n",
        "\n",
        "#df_norm = df_norm.drop(columns=['edad'])\n",
        "\n",
        "# Reemplazar los valores NaN por 'Desconocido' en Genero\n",
        "df_norm[\"Genero\"].fillna('Desconocido', inplace=True)\n",
        "\n",
        "\n",
        "# Reemplazar los valores NaN por 'Desconocido' en Edad\n",
        "#df_norm[\"Edad\"].fillna('Desconocido', inplace=True)\n",
        "\n",
        "X = df_norm.drop(\"target\", axis=1)\n",
        "y = df_norm[\"target\"]\n",
        "\n",
        "X_train_norm, X_test_norm, y_train, y_test = train_test_split(X, y, random_state=3, test_size=0.3)\n",
        "\n",
        "\n",
        "pl = Pipeline(steps=[\n",
        "    (\"LVLImputer\", ColImputer(imputer=SimpleImputer(strategy='mean'), columns=[\"LVL\"])),\n",
        "    (\"EdadImputer\", ColImputer(imputer=SimpleImputer(strategy='mean'), columns=[\"Edad\"])),\n",
        "    (\"LaboralDummies\", ColDummy(columns=[\"Laboral\"])),\n",
        "    (\"GeneroDummies\", ColDummy(columns=[\"Genero\"], delete='Genero_Desconocido')),\n",
        "    (\"BLD03Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD03\"])),\n",
        "    (\"BLD02Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD02\"])),\n",
        "    (\"BLD01Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD01\"])),\n",
        "    (\"LVLScaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"LVL\"]))\n",
        "])\n",
        "\n",
        "pl.fit(X_train_norm, y_train)\n",
        "X_train = pl.transform(X_train_norm)\n",
        "X_test = pl.transform(X_test_norm)\n",
        "\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Armado de modelos y obtencion de precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9Zjr34cZzOD"
      },
      "outputs": [],
      "source": [
        "logreg = LogisticRegression( max_iter=3000 )\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_lg = logreg.predict(X_test)\n",
        "\n",
        "treeclf = DecisionTreeClassifier(max_depth=10, random_state=1)\n",
        "treeclf.fit(X_train, y_train)\n",
        "y_pred_tc = treeclf.predict(X_test)\n",
        "\n",
        "bayes_multi = MultinomialNB()\n",
        "bayes_multi.fit(X_train, y_train)\n",
        "y_pred_nb = bayes_multi.predict(X_test)\n",
        "\n",
        "bayes_gauss = GaussianNB()\n",
        "bayes_gauss.fit(X_train, y_train)\n",
        "y_pred_gauss = bayes_gauss.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "6V5dcadcZ37E",
        "outputId": "5cab3000-d227-4a97-a982-1f0bfa616cdb"
      },
      "outputs": [],
      "source": [
        "# Inicializamos los labels del gráfico\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.xlabel('% 1 – Specificity (falsos positivos)', fontsize=14)\n",
        "plt.ylabel('% Sensitivity (positivos)', fontsize=14)\n",
        "\n",
        "# Graficamos la recta del azar\n",
        "it = [i/100 for i in range(100)]\n",
        "plt.plot(it,it,label=\"AZAR AUC=0.5\",color=\"black\")\n",
        "\n",
        "modelos = { 'bayesGauss':y_pred_gauss,'arbol':y_pred_tc, 'reglog':y_pred_lg, 'multinomial': y_pred_nb}\n",
        "areas = []\n",
        "for pred in modelos:\n",
        "    auc = graficarCurvaRoc( modelos[pred] , pred )\n",
        "    areas.append( (pred, auc) )\n",
        "areas = pd.DataFrame(areas, columns=['model','auc'])\n",
        "# Grafico\n",
        "# plt.title(\"Curva ROC\", fontsize=14)\n",
        "# plt.tick_params(labelsize=12);\n",
        "# plt.show()\n",
        "# Tabla\n",
        "areas.sort_values('auc', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKGo9WqqaMbT"
      },
      "source": [
        "### Clasificacion de todos los modelos.\n",
        "Debemos mejorar precision: para evitar los falsos positivos, dado que el tratamiento es invasivo y la gravedad moderada.\n",
        "Debemos mejorar recall: porque queremos detectar todos los positivos posibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wsj5VGM5aPEc",
        "outputId": "4915383f-f866-4159-ee0d-f27acea0dc76"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(f\"## LogisticRegression\"))\n",
        "print(classification_report(y_test, y_pred_lg))\n",
        "\n",
        "display(Markdown(f\"## DecisionTreeClassifier\"))\n",
        "print(classification_report(y_test, y_pred_tc))\n",
        "\n",
        "display(Markdown(f\"## MultinomialNB\"))\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "\n",
        "display(Markdown(f\"## GaussianNB\"))\n",
        "print(classification_report(y_test, y_pred_gauss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mejora de los modelos con GridSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from warnings import simplefilter\n",
        "# ignore all warnings\n",
        "simplefilter(action='ignore')\n",
        "\n",
        "# Con todas las opciones de solver no converge nunca\n",
        "# \"solver\": ['saga', 'sag', 'newton-cholesky', 'newton-cg', 'liblinear', 'lbfgs']\n",
        "\n",
        "parameters =  {\"C\":np.logspace(-3,3,13), \"penalty\":[\"l1\",\"l2\",None], \"max_iter\":[1500,3000,4000]}\n",
        "clf = GridSearchCV( LogisticRegression() , parameters, scoring='precision', cv=5, )\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", clf.best_params_)\n",
        "print(\"Best Score:\", clf.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = {\"criterion\": ['gini', 'entropy', 'log_loss'], \"splitter\": ['best', 'random'], \"max_depth\": [2, 5, 10, 20, 30, 40], \"random_state\": [1,5,9,15,20,30,40]}\n",
        "clf = GridSearchCV( DecisionTreeClassifier() , parameters, scoring='precision', cv=5, )\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", clf.best_params_)\n",
        "print(\"Best Score:\", clf.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multinomial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = {\"fit_prior\": [True, False], \"alpha\": [0,1.0,2.0,3.0], \"force_alpha\": [True, False]}\n",
        "clf = GridSearchCV( MultinomialNB() , parameters, scoring='precision', cv=5, )\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", clf.best_params_)\n",
        "print(\"Best Score:\", clf.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "clf = GridSearchCV( GaussianNB() , parameters, scoring='precision', cv=5, )\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", clf.best_params_)\n",
        "print(\"Best Score:\", clf.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Re-Evaluacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(max_iter=1500, penalty='l2', C=np.float64(0.31622776601683794))\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_lg = logreg.predict(X_test)\n",
        "\n",
        "\n",
        "treeclf = DecisionTreeClassifier(max_depth=5, random_state=20, splitter='random', criterion='entropy')\n",
        "treeclf.fit(X_train, y_train)\n",
        "y_pred_tc = treeclf.predict(X_test)\n",
        "\n",
        "bayes_multi = MultinomialNB(alpha=2.0, fit_prior=True, force_alpha=True)\n",
        "bayes_multi.fit(X_train, y_train)\n",
        "y_pred_nb = bayes_multi.predict(X_test)\n",
        "\n",
        "bayes_gauss = GaussianNB(var_smoothing=np.float64(1.0))\n",
        "bayes_gauss.fit(X_train, y_train)\n",
        "y_pred_gauss = bayes_gauss.predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.xlabel('% 1 – Specificity (falsos positivos)', fontsize=14)\n",
        "plt.ylabel('% Sensitivity (positivos)', fontsize=14)\n",
        "\n",
        "# Graficamos la recta del azar\n",
        "it = [i/100 for i in range(100)]\n",
        "plt.plot(it,it,label=\"AZAR AUC=0.5\",color=\"black\")\n",
        "\n",
        "modelos = { 'bayesGauss':y_pred_gauss,'arbol':y_pred_tc, 'reglog':y_pred_lg, 'multinomial': y_pred_nb}\n",
        "areas = []\n",
        "for pred in modelos:\n",
        "    auc = graficarCurvaRoc( modelos[pred] , pred )\n",
        "    areas.append( (pred, auc) )\n",
        "areas = pd.DataFrame(areas, columns=['model','auc'])\n",
        "# Grafico\n",
        "# plt.title(\"Curva ROC\", fontsize=14)\n",
        "# plt.tick_params(labelsize=12);\n",
        "# plt.show()\n",
        "# Tabla\n",
        "areas.sort_values('auc', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(f\"## LogisticRegression\"))\n",
        "print(classification_report(y_test, y_pred_lg))\n",
        "\n",
        "display(Markdown(f\"## DecisionTreeClassifier\"))\n",
        "print(classification_report(y_test, y_pred_tc))\n",
        "\n",
        "display(Markdown(f\"## MultinomialNB\"))\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "\n",
        "display(Markdown(f\"## GaussianNB\"))\n",
        "print(classification_report(y_test, y_pred_gauss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validación\n",
        "\n",
        "Para garantizar la robustez y la generalización de nuestro modelo utilizaremos la técnica de validación cruzada (K-Folds). Esto nos permitirá evaluar el rendimiento del modelo de manera más exhaustiva.\n",
        "\n",
        "La validación cruzada nos ayudará a reducir el sesgo de evaluación,Además nos permitirá identificar la variabilidad en el rendimiento del modelo, minimizando así el riesgo de sobreajuste (overfitting) y asegurando que el modelo generalice bien a nuevos datos no vistos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def realizar_kfolds_val(model,X,y,splits=5,shf=True):\n",
        "    kf = KFold(n_splits=splits, shuffle=shf, random_state=42)\n",
        "\n",
        "    # Evaluar el modelo utilizando validación cruzada\n",
        "    scores = cross_val_score(model, X, y, cv=kf)\n",
        "\n",
        "    print(f'Scores for each fold: {scores}')\n",
        "    print(f'Mean score: {scores.mean()}')\n",
        "    print(f'Standard deviation: {scores.std()}')\n",
        "\n",
        "print(\"Cross-Validation - Log Reg\")\n",
        "realizar_kfolds_val(logreg,X_train,y_train,5)\n",
        "\n",
        "print(\"\\n\\nCross-Validation - Decision Tree Classifier\")\n",
        "realizar_kfolds_val(treeclf,X_train,y_train,5)\n",
        "\n",
        "print(\"\\n\\nCross-Validation - Multinomial Bayes\")\n",
        "realizar_kfolds_val(bayes_multi,X_train,y_train,5)\n",
        "\n",
        "print(\"\\n\\nCross-Validation - Gaussian Bayes\")\n",
        "realizar_kfolds_val(bayes_gauss,X_train,y_train,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pendientes:\n",
        "\n",
        "Ver si podemos balancer la variable target, (todas las metricas nos dab bajas para el caso de positivos)\n",
        "Tratar las variables faltantes solo estamos haciendo:\n",
        "(\"LVLImputer\", ColImputer(imputer=SimpleImputer(strategy='mean'), columns=[\"LVL\"])),\n",
        "    (\"EdadImputer\", ColImputer(imputer=SimpleImputer(strategy='mean'), columns=[\"Edad\"])),\n",
        "    (\"LaboralDummies\", ColDummy(columns=[\"Laboral\"])),\n",
        "    (\"GeneroDummies\", ColDummy(columns=[\"Genero\"], delete='Genero_Desconocido')),\n",
        "    (\"BLD03Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD03\"])),\n",
        "    (\"BLD02Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD02\"])),\n",
        "    (\"BLD01Scaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"BLD01\"])),\n",
        "    (\"LVLScaler\", ColScaler(scaler=MinMaxScaler(), columns=[\"LVL\"]))\n",
        "Probar otros scalers, strategias etc...\n",
        "Ver validacion Cruzada\n",
        "Ver sesgo de seleccion y fuga de datos\n",
        "\n",
        "en la parte de tratamiento de variables no estamos haciendo nada relevante, hace falta ese scaler a todo el dataset? o podemos tener los scalers del pipeline y ya?\n",
        "por lo que veo en los colab escala casi siempre todas las variables creo deberiamos hacerlo\n",
        "\n",
        "en varios colab elige que variables utilizar y cuales no (en base a la correlacion?) ver si eso mejora las metricas"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
